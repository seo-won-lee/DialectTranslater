# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import pandas as pd
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import matplotlib.pyplot as plt

# âœ… GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ë¡œì»¬ ê²½ë¡œë¡œ ìˆ˜ì •)
data = pd.read_csv('./final_data.csv', encoding='ISO-8859-1')

# âœ… ë°ì´í„° ë‚˜ëˆ„ê¸° (Train: 80%, Validation: 20%)
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)
print(f"Train Data í¬ê¸°: {len(train_data)}")
print(f"Validation Data í¬ê¸°: {len(val_data)}")

# âœ… TranslationDataset í´ë˜ìŠ¤ ì •ì˜
class TranslationDataset(Dataset):
    def __init__(self, data, tokenizer, max_len=128):
        self.data = data.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_len = max_len

        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        required_columns = ['source', 'source_word', 'country', 'meaning', 'target']
        missing_columns = [col for col in required_columns if col not in self.data.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        row = self.data.iloc[index]
        source_text = str(row['source']).strip()
        masked_word = str(row['source_word']).strip()
        country = str(row['country']).strip()
        meaning = str(row['meaning']).strip()
        target_text = str(row['target']).strip()

        # mask ì²˜ë¦¬
        if masked_word and masked_word in source_text:
            masked_source = source_text.replace(masked_word, "<mask>")
        else:
            masked_source = source_text

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        source_text = f"Translate this sentence to {country} English, considering the meaning '{meaning}': {masked_source}"

        # ì¸ì½”ë”©
        source_encoding = self.tokenizer(
            source_text,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        target_encoding = self.tokenizer(
            target_text,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # ë ˆì´ë¸” ì²˜ë¦¬
        labels = target_encoding['input_ids'].clone()
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': source_encoding['input_ids'].squeeze(0),
            'attention_mask': source_encoding['attention_mask'].squeeze(0),
            'labels': labels.squeeze(0)
        }

    @staticmethod
    def collate_fn(batch):
        input_ids = torch.stack([item['input_ids'] for item in batch])
        attention_mask = torch.stack([item['attention_mask'] for item in batch])
        labels = torch.stack([item['labels'] for item in batch])

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

# âœ… ë°ì´í„°ë¡œë” ìƒì„± í•¨ìˆ˜
def create_data_loaders(train_data, val_data, tokenizer, batch_size, max_len=128):
    train_dataset = TranslationDataset(train_data, tokenizer, max_len)
    val_dataset = TranslationDataset(val_data, tokenizer, max_len)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=TranslationDataset.collate_fn,
        num_workers=0,
        drop_last=False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=TranslationDataset.collate_fn,
        num_workers=0,
        drop_last=False
    )

    return train_loader, val_loader

# âœ… tokenizerì™€ ëª¨ë¸ ë¡œë“œ
tokenizer = T5Tokenizer.from_pretrained("t5-base")
model = T5ForConditionalGeneration.from_pretrained("t5-base")

# âœ… ëª¨ë¸ì„ GPUë¡œ ì´ë™
model.to(device)
print("Model loaded and moved to device.")

# âœ… ë°ì´í„°ë¡œë” ìƒì„±
train_loader, val_loader = create_data_loaders(train_data, val_data, tokenizer, batch_size=16)

# âœ… ë°ì´í„° í™•ì¸
for batch in train_loader:
    print("Batch input IDs shape:", batch['input_ids'].shape)
    print("Batch labels shape:", batch['labels'].shape)
    break


# ë°ì´í„°í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ë¥¼ ë¦¬ì…‹
train_data = train_data.reset_index(drop=True)
val_data = val_data.reset_index(drop=True)

# í•˜ì´í¼ íŒŒë¼ë¯¸í„° í›„ë³´
learning_rates = [1e-5, 1e-4, 1e-3]
num_epochs_list = [5, 10, 15]
batch_sizes = [2, 4, 8]  # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°

best_val_loss = float("inf")  # ë¬´í•œëŒ€ë¡œ ì´ˆê¸°í™”
best_params = None

# ì†ì‹¤ ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
all_train_losses = []
all_val_losses = []

# Gradient Accumulation ì„¤ì •
gradient_accumulation_steps = 2

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë£¨í”„
for batch_size in batch_sizes:
    for learning_rate in learning_rates:
        for num_epochs in num_epochs_list:
            print(f"\nğŸš€ Training with Batch Size: {batch_size}, Learning Rate: {learning_rate}, Epochs: {num_epochs}")

            # ë°ì´í„°ë¡œë” ìƒì„±
            train_loader, val_loader = create_data_loaders(
                train_data=train_data,
                val_data=val_data,
                tokenizer=tokenizer,
                batch_size=batch_size,
                max_len=64  # í† í° ìµœëŒ€ ê¸¸ì´ ì¤„ì´ê¸°
            )

            # ì˜µí‹°ë§ˆì´ì € ì •ì˜
            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

            # ì†ì‹¤ ê°’ ì´ˆê¸°í™”
            train_losses = []
            val_losses = []

            # í›ˆë ¨ ë£¨í”„
            for epoch in range(num_epochs):
                model.train()
                train_loss = 0

                for step, batch in enumerate(tqdm(train_loader, desc=f"Training Epoch {epoch+1}")):
                    input_ids = batch["input_ids"].to(device)
                    attention_mask = batch["attention_mask"].to(device)
                    labels = batch["labels"].to(device)

                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss
                    train_loss += loss.item()

                    # Gradient ëˆ„ì 
                    loss = loss / gradient_accumulation_steps
                    loss.backward()

                    if (step + 1) % gradient_accumulation_steps == 0:
                        optimizer.step()
                        optimizer.zero_grad()

                # í‰ê·  Train Loss ì €ì¥
                avg_train_loss = train_loss / len(train_loader)
                train_losses.append(avg_train_loss)

                # ê²€ì¦ ë£¨í”„
                model.eval()
                val_loss = 0
                with torch.no_grad():
                    for batch in val_loader:
                        input_ids = batch["input_ids"].to(device)
                        attention_mask = batch["attention_mask"].to(device)
                        labels = batch["labels"].to(device)

                        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                        loss = outputs.loss
                        val_loss += loss.item()

                # í‰ê·  Validation Loss ì €ì¥
                avg_val_loss = val_loss / len(val_loader)
                val_losses.append(avg_val_loss)

                # ì†ì‹¤ ì¶œë ¥
                print(f"Epoch {epoch + 1}, Avg Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}")

                # ìµœì ì˜ ê²€ì¦ ì†ì‹¤ ê°’ ì €ì¥
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    best_params = {"batch_size": batch_size, "learning_rate": learning_rate, "num_epochs": num_epochs}

            # ëª¨ë“  ì†ì‹¤ ê°’ ì €ì¥
            all_train_losses.append((batch_size, learning_rate, num_epochs, train_losses))
            all_val_losses.append((batch_size, learning_rate, num_epochs, val_losses))

            # âœ… ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()

# ìµœì ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¶œë ¥
print("\nâœ… ìµœì ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°í•©:")
print(best_params)
print(f"ìµœì†Œ Validation Loss: {best_val_loss:.4f}")

# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
for params, train_loss in all_train_losses:
    batch_size, learning_rate, num_epochs, losses = params
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(losses) + 1), losses, label=f"Train Loss (Batch: {batch_size}, LR: {learning_rate}, Epochs: {num_epochs})")
    plt.title(f"Train Loss for Batch Size {batch_size}, Learning Rate {learning_rate}, Epochs {num_epochs}")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.show()

for params, val_loss in all_val_losses:
    batch_size, learning_rate, num_epochs, losses = params
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(losses) + 1), losses, label=f"Validation Loss (Batch: {batch_size}, LR: {learning_rate}, Epochs {num_epochs})")
    plt.title(f"Validation Loss for Batch Size {batch_size}, Learning Rate {learning_rate}, Epochs {num_epochs}")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.show()





